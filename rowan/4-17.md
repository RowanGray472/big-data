---
Class: Big Data
---

1. our next hw assignment builds off of this one
    1. after we do this assignnment we can really efficeintly load data
        1. so next time we can do beeg data
    2. should only take an hour or two to parallel insert the data
2. productive programmers don't have better brain power
    1. they just have better habits- doing things like hardcoding version numbers
        1. also regularly post to github!
    2. it seems like programming slower is faster than programming fast
3. be in the habit of using docker and git for everything!
4. indexes
    1. reduces runtime for finding a row from O(n) to theta(log n)
        1. worst case time of n to every time log n
        2. hugeeee performance gain
    2. disadvantage- decreases insert from 1 to logn
    3. these solve most sql performance problems and most industry people don't get it
    4. this is what we've been building up to all semester
5. built in indices
    1. BTree- what we're doing
    2. Gin- for full text search
    3. RUM - imprved version
    4. ivfflat - very popular with llms for vector embeddings
6. vocab thing!
    1. 'access method' == index
7. btrees
    1. an extension of the balanced binary search trees- remember these from data structures?
    2. this is optimized for on-disk storage
    3. it's fast to find information close together, slow to seek information further apart- the b tree is structured around this
8. hard drive spins at 7200 rpm on a server- laptops are slower
9. recording head moves back and forth- doesn't move very fast
10. this is why sequential data is easier- takes advantage of the fast part of the drive, not the slow part
11. norvig.com link
12. 1 nanosec for instruction execution on a modern computer
13. one mp sequentially is 250k nanoseconds
14. fetching a new disloctaion is 8,000,000 nanoseconds
15. reading 1mb from disk is 25 mil
16. you can only read or write one section at a time- they used to have 8kb, now it's more like 4kb
17. more sequential infomration is on the outside bc thre are mor esections there
18. ssds are a physical chip- b trees aren't as necessary for this
19. everyone in the dat acenter uses hdds because they're still so much cheaper
20. the platters have ferous materials inside them with a shiny layer on top- the recording thing has a small electromagnet
21. recording head has like micron level precision
22. hdds were invented in the 50s
    1. nobody knew how to get a nitrogen-tight seal instead of just an airtight seal
    2. they just figure it out in the last 2-3 years
23. lots of different types of btrees
    1. doc is terse
    2. so our reference is different- habr.com, it's russian reddit
24. postgres doesn't use github but it does use git
25. the really good documentation isn't their website, it's on their git repo- but assumes a worknig knowledge of c
26. every btree has at least one page that stores metainfo
27. we'll have like 100-1000 inside each block- this is called the 'fanout'
28. algorithm- go to the largest value lower than it and go down
29. leftmost tuple always matches the number we're coming from
30. there are always links to adjacent nodes- to find more you don't have to retrace
31. leaves always have fewer tuples- internal blocks only store an individual number, doesn't show there it's physically stored
32. the leafs store number and tuple id
33. basic idea
    1. select statements tell us what we want
    2. query planner- decides how to calculate your select statements- rewrites your requries in imperative code
    3. means doing left joins or cte or whatever doesn't actually matter
    4. we can prove it isn't always optimal- but under regularity assumptions it is
    5. to actually understand query planning you need a compilers course
    6. `EXPLAIN` command - using this in lab tmrw
        1. explains what alg postgres is using
        2. this syntax is unique to postgres
    7. we have soem strategies to use indexes
    8. will have to use our understanding of algorithms for this though
